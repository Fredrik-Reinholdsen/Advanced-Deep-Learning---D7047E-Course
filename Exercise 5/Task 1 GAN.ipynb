{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting ../../MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting ../../MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "Iter-0; D_loss: 1.4721786975860596; G_loss: 2.8150157928466797\n",
      "Iter-1000; D_loss: 0.008873595856130123; G_loss: 6.110750675201416\n",
      "Iter-2000; D_loss: 0.06028791889548302; G_loss: 5.451226711273193\n",
      "Iter-3000; D_loss: 0.08187650889158249; G_loss: 4.765238285064697\n",
      "Iter-4000; D_loss: 0.10037525743246078; G_loss: 5.05961799621582\n",
      "Iter-5000; D_loss: 0.2604025900363922; G_loss: 4.008050441741943\n",
      "Iter-6000; D_loss: 0.7070590257644653; G_loss: 3.1169989109039307\n",
      "Iter-7000; D_loss: 0.787691593170166; G_loss: 3.6690807342529297\n",
      "Iter-8000; D_loss: 0.9096466898918152; G_loss: 3.542663097381592\n",
      "Iter-9000; D_loss: 0.623664140701294; G_loss: 2.981926918029785\n",
      "Iter-10000; D_loss: 0.7541742920875549; G_loss: 2.6150896549224854\n",
      "Iter-11000; D_loss: 0.7624431848526001; G_loss: 2.2879769802093506\n",
      "Iter-12000; D_loss: 0.552593469619751; G_loss: 3.1081955432891846\n",
      "Iter-13000; D_loss: 0.8781013488769531; G_loss: 2.054253339767456\n",
      "Iter-14000; D_loss: 0.8009483218193054; G_loss: 2.331578254699707\n",
      "Iter-15000; D_loss: 0.8269719481468201; G_loss: 2.352829694747925\n",
      "Iter-16000; D_loss: 0.7718572616577148; G_loss: 2.093147039413452\n",
      "Iter-17000; D_loss: 0.9572382569313049; G_loss: 1.68013596534729\n",
      "Iter-18000; D_loss: 0.6856703758239746; G_loss: 2.3845362663269043\n",
      "Iter-19000; D_loss: 1.0415207147598267; G_loss: 2.0931239128112793\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "import tensorflow as tf\n",
    "import input_data\n",
    "\n",
    "#from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('../../MNIST_data', one_hot=True)\n",
    "mb_size = 64\n",
    "Z_dim = 100\n",
    "X_dim = mnist.train.images.shape[1]\n",
    "y_dim = mnist.train.labels.shape[1]\n",
    "h_dim = 128\n",
    "c = 0\n",
    "lr = 1e-3\n",
    "\n",
    "\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / np.sqrt(in_dim / 2.)\n",
    "    return Variable(torch.randn(*size) * xavier_stddev, requires_grad=True)\n",
    "\n",
    "\n",
    "\"\"\" ==================== GENERATOR ======================== \"\"\"\n",
    "\n",
    "Wzh = xavier_init(size=[Z_dim, h_dim])\n",
    "bzh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
    "\n",
    "Whx = xavier_init(size=[h_dim, X_dim])\n",
    "bhx = Variable(torch.zeros(X_dim), requires_grad=True)\n",
    "\n",
    "\n",
    "def G(z):\n",
    "    h = nn.relu(z @ Wzh + bzh.repeat(z.size(0), 1))\n",
    "    X = torch.sigmoid(h @ Whx + bhx.repeat(h.size(0), 1))\n",
    "    return X\n",
    "\n",
    "\n",
    "\"\"\" ==================== DISCRIMINATOR ======================== \"\"\"\n",
    "\n",
    "Wxh = xavier_init(size=[X_dim, h_dim])\n",
    "bxh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
    "\n",
    "Why = xavier_init(size=[h_dim, 1])\n",
    "bhy = Variable(torch.zeros(1), requires_grad=True)\n",
    "\n",
    "\n",
    "def D(X):\n",
    "    h = nn.relu(X @ Wxh + bxh.repeat(X.size(0), 1))\n",
    "    y = nn.sigmoid(h @ Why + bhy.repeat(h.size(0), 1))\n",
    "    return y\n",
    "\n",
    "\n",
    "G_params = [Wzh, bzh, Whx, bhx]\n",
    "D_params = [Wxh, bxh, Why, bhy]\n",
    "params = G_params + D_params\n",
    "\n",
    "\n",
    "\"\"\" ===================== TRAINING ======================== \"\"\"\n",
    "\n",
    "\n",
    "def reset_grad():\n",
    "    for p in params:\n",
    "        if p.grad is not None:\n",
    "            data = p.grad.data\n",
    "            p.grad = Variable(data.new().resize_as_(data).zero_())\n",
    "\n",
    "\n",
    "G_solver = optim.Adam(G_params, lr=1e-3)\n",
    "D_solver = optim.Adam(D_params, lr=1e-3)\n",
    "\n",
    "ones_label = Variable(torch.ones(mb_size, 1))\n",
    "zeros_label = Variable(torch.zeros(mb_size, 1))\n",
    "loss_type = 'logits'\n",
    "\n",
    "for it in range(20000):\n",
    "    # Sample data\n",
    "    z = Variable(torch.randn(mb_size, Z_dim))\n",
    "    X, _ = mnist.train.next_batch(mb_size)\n",
    "    X = Variable(torch.from_numpy(X))\n",
    "\n",
    "    # Dicriminator forward-loss-backward-update\n",
    "    if loss_type == 'logits':\n",
    "        \n",
    "        G_sample = G(z)\n",
    "        D_real = D(X)\n",
    "        D_fake = D(G_sample)\n",
    "\n",
    "        D_loss_real = nn.binary_cross_entropy(D_real, ones_label)\n",
    "        D_loss_fake = nn.binary_cross_entropy(D_fake, zeros_label)\n",
    "        D_loss = D_loss_real + D_loss_fake\n",
    "\n",
    "        D_loss.backward()\n",
    "        D_solver.step()\n",
    "\n",
    "        # Housekeeping - reset gradient\n",
    "        reset_grad()\n",
    "\n",
    "        # Generator forward-loss-backward-update\n",
    "        z = Variable(torch.randn(mb_size, Z_dim))\n",
    "        G_sample = G(z)\n",
    "        D_fake = D(G_sample)\n",
    "\n",
    "        G_loss = nn.binary_cross_entropy(D_fake, ones_label)\n",
    "\n",
    "        G_loss.backward()\n",
    "        G_solver.step()\n",
    "        \n",
    "    elif loss_type == 'goodfellow':\n",
    "        G_sample = G(z)\n",
    "        D_real = D(X)\n",
    "        D_fake = D(G_sample) \n",
    "        D_loss = -torch.mean(torch.log(D_real) + torch.log(1. - D_fake))\n",
    "        G_loss = -torch.mean(torch.log(D_fake))\n",
    "        D_loss.backward(retain_graph=True)\n",
    "        D_solver.step()\n",
    "        G_loss.backward(retain_graph=True)\n",
    "        G_solver.step()\n",
    "    # Housekeeping - reset gradient\n",
    "    reset_grad()\n",
    "\n",
    "    # Print and plot every now and then\n",
    "    if it % 1000 == 0:\n",
    "        print('Iter-{}; D_loss: {}; G_loss: {}'.format(it, D_loss.data.numpy(), G_loss.data.numpy()))\n",
    "\n",
    "        samples = G(z).data.numpy()[:16]\n",
    "\n",
    "        fig = plt.figure(figsize=(4, 4))\n",
    "        gs = gridspec.GridSpec(4, 4)\n",
    "        gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "        for i, sample in enumerate(samples):\n",
    "            ax = plt.subplot(gs[i])\n",
    "            plt.axis('off')\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "            ax.set_aspect('equal')\n",
    "            plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "        if not os.path.exists('out/'):\n",
    "            os.makedirs('out/')\n",
    "\n",
    "        plt.savefig('out/{}.png'.format(str(c).zfill(3)), bbox_inches='tight')\n",
    "        c += 1\n",
    "        plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
