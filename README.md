## Advanced-Deep-Learning---D7047E-Course

# Exercise 1: Setting up and using DeepDIVA
1. Download and prepare the CIFAR10 dataset, command: python util/data/get_a_dataset.py --dataset CIFAR-10 --output-folder toy_datasets

2. Run Image Classification with seed 42, command: python template/RunMe.py --dataset-folder toy_datasets/CIFAR10 --seed42 --ignoregit --no-cuda. Resulting Accuracy: 23.11 %

3. Run previous exercise with Adam optimizer, command: python template/RunMe.py --dataset-folder toy_datasets/CIFAR10 --seed42 --ignoregit --no-cuda --optimizer-name Adam. Resulting Accuracy: 60.45 %

4. Run previous exercise with Adam optimizer, command: python template/RunMe.py --dataset-folder toy_datasets/CIFAR10 --seed42 --ignoregit --no-cuda --optimizer-name Adam --model-name CNN_basic_copy. Resulting Accuracy: 52.94 %

# Exercise 3: t-SNE vs. PCA
In this exercise a basic CNN model has been trained for 10 epochs on the MNIST dataset. Then PCA and t-SNE have been applied to visualize what the network has learned

#Exercise 4: char - RNN
Output “2 b3n”: 
2 b3ne is not that your divising the proceed:
I' the wear be these, I know preceded speed:
It is a sated

Output “bg09Z”: 
bg09ZI will all ungelold durnal;
For he spies for an infire, she dead:
And say me round to whom he look:

Output “qwert”: 
qwert a place and cell, do that?

First Yithart:
Make the sake. My lords of the trumpet of brined,
That t

Output “The”: 
They wherefore men and most ender. Therefore o'
shephembers stopp'd with the strangely senses!

Output “What is”: 
What is these to before us:
The eyes; so that shamelest saw galle the basely.
Here of letter before the blo

Output “Shall I give”: 
Shall I given's rumperation to the sleep.
Simes, where is seo so doth thus to are
are the safety sare of the oth

Output “X087hNYB BHN BYFVuhsdbs”: 
X087hNYB BHN BYFVuhsdbstroy, where you may blow, request,
For my process, for that they pinnall thee.
Like the company, tho

Loss at every 100 epochs: 
1.7783 , 1.5726, 1.4908, 1.4732, 1.3992, 1.4017, 1.4027, 1.3851, 1.3655, 1.3808, 1.3592, 1.3729, 1.3436, 1.3512, 1.3655, 1.3702, 1.3427, 1.3499, 1.3502, 1.3524

